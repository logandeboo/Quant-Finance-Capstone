{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56425,"status":"ok","timestamp":1678997449652,"user":{"displayName":"Logan Deboo","userId":"05593846818062128905"},"user_tz":420},"id":"CL-iOLVynZjS","outputId":"9bf16abb-9d7e-4f56-bcba-b11f153acafb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting huggingface\n","  Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n","Installing collected packages: huggingface\n","Successfully installed huggingface-0.0.1\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sec-api\n","  Downloading sec_api-1.0.15-py3-none-any.whl (15 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from sec-api) (2.27.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->sec-api) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->sec-api) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->sec-api) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->sec-api) (2022.12.7)\n","Installing collected packages: sec-api\n","Successfully installed sec-api-1.0.15\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pynvml\n","  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pynvml\n","Successfully installed pynvml-11.5.0\n"]}],"source":["!pip install huggingface\n","!pip -q install -q nltk transformers\n","!pip install sec-api\n","!pip install pynvml"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21840,"status":"ok","timestamp":1678997471488,"user":{"displayName":"Logan Deboo","userId":"05593846818062128905"},"user_tz":420},"id":"LPkqbf1bedhK","outputId":"d4b495d2-9afa-4ea6-d360-1678b3842282"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["import pandas as pd\n","import pandas_datareader as web\n","from google.colab import drive\n","import requests\n","import os\n","import nltk\n","from transformers import pipeline\n","nltk.download('punkt')\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import time\n","import threading\n","import re\n","from datetime import datetime\n","from sec_api import QueryApi\n","from datetime import datetime, timedelta\n","from pynvml import *\n","from sec_api import QueryApi\n","from sec_api import ExtractorApi\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from dateutil.parser import parse\n","import statsmodels.api as sm\n","import statsmodels.api as sm\n","import pandas as pd\n","from google.colab import drive\n","import scipy.stats as stats\n","import numpy as np\n","from scipy.stats.mstats import winsorize\n","import matplotlib.pyplot as plt\n","from datetime import datetime, timedelta\n","from dateutil.relativedelta import relativedelta\n","import seaborn as sns\n","import nltk"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"-xTaWsuKfwTv","executionInfo":{"status":"ok","timestamp":1682316765846,"user_tz":420,"elapsed":1,"user":{"displayName":"Logan Deboo","userId":"05593846818062128905"}}},"outputs":[],"source":["#@title S&P-500 Ticker List\n","\n","total_SP = ['A',\n"," 'AAL',\n"," 'AAP',\n"," 'AAPL',\n"," 'ABBV',\n"," 'ABC',\n"," 'ABT',\n"," 'ACGL',\n"," 'ACN',\n"," 'ADBE',\n"," 'ADI',\n"," 'ADM',\n"," 'ADP',\n"," 'ADSK',\n"," 'AEE',\n"," 'AEP',\n"," 'AES',\n"," 'AFL',\n"," 'AIG',\n"," 'AIZ',\n"," 'AJG',\n"," 'AKAM',\n"," 'ALB',\n"," 'ALGN',\n"," 'ALK',\n"," 'ALL',\n"," 'ALLE',\n"," 'AMAT',\n"," 'AMCR',\n"," 'AMD',\n"," 'AME',\n"," 'AMGN',\n"," 'AMP',\n"," 'AMT',\n"," 'AMZN',\n"," 'ANET',\n"," 'ANSS',\n"," 'AON',\n"," 'AOS',\n"," 'APA',\n"," 'APD',\n"," 'APH',\n"," 'APTV',\n"," 'ARE',\n"," 'ATO',\n"," 'ATVI',\n"," 'AVB',\n"," 'AVGO',\n"," 'AVY',\n"," 'AWK',\n"," 'AXP',\n"," 'AZO',\n"," 'BA',\n"," 'BAC',\n"," 'BALL',\n"," 'BAX',\n"," 'BBWI',\n"," 'BBY',\n"," 'BDX',\n"," 'BEN',\n"," 'BF.B',\n"," 'BIIB',\n"," 'BIO',\n"," 'BK',\n"," 'BKNG',\n"," 'BKR',\n"," 'BLK',\n"," 'BMY',\n"," 'BR',\n"," 'BRK.B',\n"," 'BRO',\n"," 'BSX',\n"," 'BWA',\n"," 'BXP',\n"," 'C',\n"," 'CAG',\n"," 'CAH',\n"," 'CARR',\n"," 'CAT',\n"," 'CB',\n"," 'CBOE',\n"," 'CBRE',\n"," 'CCI',\n"," 'CCL',\n"," 'CDAY',\n"," 'CDNS',\n"," 'CDW',\n"," 'CE',\n"," 'CEG',\n"," 'CF',\n"," 'CFG',\n"," 'CHD',\n"," 'CHRW',\n"," 'CHTR',\n"," 'CI',\n"," 'CINF',\n"," 'CL',\n"," 'CLX',\n"," 'CMA',\n"," 'CMCSA',\n"," 'CME',\n"," 'CMG',\n"," 'CMI',\n"," 'CMS',\n"," 'CNC',\n"," 'CNP',\n"," 'COF',\n"," 'COO',\n"," 'COP',\n"," 'COST',\n"," 'CPB',\n"," 'CPRT',\n"," 'CPT',\n"," 'CRL',\n"," 'CRM',\n"," 'CSCO',\n"," 'CSGP',\n"," 'CSX',\n"," 'CTAS',\n"," 'CTLT',\n"," 'CTRA',\n"," 'CTSH',\n"," 'CTVA',\n"," 'CVS',\n"," 'CVX',\n"," 'CZR',\n"," 'D',\n"," 'DAL',\n"," 'DD',\n"," 'DE',\n"," 'DFS',\n"," 'DG',\n"," 'DGX',\n"," 'DHI',\n"," 'DHR',\n"," 'DIS',\n"," 'DISH',\n"," 'DLR',\n"," 'DLTR',\n"," 'DOV',\n"," 'DOW',\n"," 'DPZ',\n"," 'DRI',\n"," 'DTE',\n"," 'DUK',\n"," 'DVA',\n"," 'DVN',\n"," 'DXC',\n"," 'DXCM',\n"," 'EA',\n"," 'EBAY',\n"," 'ECL',\n"," 'ED',\n"," 'EFX',\n"," 'EIX',\n"," 'EL',\n"," 'ELV',\n"," 'EMN',\n"," 'EMR',\n"," 'ENPH',\n"," 'EOG',\n"," 'EPAM',\n"," 'EQIX',\n"," 'EQR',\n"," 'EQT',\n"," 'ES',\n"," 'ESS',\n"," 'ETN',\n"," 'ETR',\n"," 'ETSY',\n"," 'EVRG',\n"," 'EW',\n"," 'EXC',\n"," 'EXPD',\n"," 'EXPE',\n"," 'EXR',\n"," 'F',\n"," 'FANG',\n"," 'FAST',\n"," 'FCX',\n"," 'FDS',\n"," 'FDX',\n"," 'FE',\n"," 'FFIV',\n"," 'FIS',\n"," 'FISV',\n"," 'FITB',\n"," 'FLT',\n"," 'FMC',\n"," 'FOX',\n"," 'FOXA',\n"," 'FRT',\n"," 'FSLR',\n"," 'FTNT',\n"," 'FTV',\n"," 'GD',\n"," 'GE',\n"," 'GEN',\n"," 'GILD',\n"," 'GIS',\n"," 'GL',\n"," 'GLW',\n"," 'GM',\n"," 'GNRC',\n"," 'GOOG',\n"," 'GOOGL',\n"," 'GPC',\n"," 'GPN',\n"," 'GRMN',\n"," 'GS',\n"," 'GWW',\n"," 'HAL',\n"," 'HAS',\n"," 'HBAN',\n"," 'HCA',\n"," 'HD',\n"," 'HES',\n"," 'HIG',\n"," 'HII',\n"," 'HLT',\n"," 'HOLX',\n"," 'HON',\n"," 'HPE',\n"," 'HPQ',\n"," 'HRL',\n"," 'HSIC',\n"," 'HST',\n"," 'HSY',\n"," 'HUM',\n"," 'HWM',\n"," 'IBM',\n"," 'ICE',\n"," 'IDXX',\n"," 'IEX',\n"," 'IFF',\n"," 'ILMN',\n"," 'INCY',\n"," 'INTC',\n"," 'INTU',\n"," 'INVH',\n"," 'IP',\n"," 'IPG',\n"," 'IQV',\n"," 'IR',\n"," 'IRM',\n"," 'ISRG',\n"," 'IT',\n"," 'ITW',\n"," 'IVZ',\n"," 'J',\n"," 'JBHT',\n"," 'JCI',\n"," 'JKHY',\n"," 'JNJ',\n"," 'JNPR',\n"," 'JPM',\n"," 'K',\n"," 'KDP',\n"," 'KEY',\n"," 'KEYS',\n"," 'KHC',\n"," 'KIM',\n"," 'KLAC',\n"," 'KMB',\n"," 'KMI',\n"," 'KMX',\n"," 'KO',\n"," 'KR',\n"," 'L',\n"," 'LDOS',\n"," 'LEN',\n"," 'LH',\n"," 'LHX',\n"," 'LIN',\n"," 'LKQ',\n"," 'LLY',\n"," 'LMT',\n"," 'LNC',\n"," 'LNT',\n"," 'LOW',\n"," 'LRCX',\n"," 'LUMN',\n"," 'LUV',\n"," 'LVS',\n"," 'LW',\n"," 'LYB',\n"," 'LYV',\n"," 'MA',\n"," 'MAA',\n"," 'MAR',\n"," 'MAS',\n"," 'MCD',\n"," 'MCHP',\n"," 'MCK',\n"," 'MCO',\n"," 'MDLZ',\n"," 'MDT',\n"," 'MET',\n"," 'META',\n"," 'MGM',\n"," 'MHK',\n"," 'MKC',\n"," 'MKTX',\n"," 'MLM',\n"," 'MMC',\n"," 'MMM',\n"," 'MNST',\n"," 'MO',\n"," 'MOH',\n"," 'MOS',\n"," 'MPC',\n"," 'MPWR',\n"," 'MRK',\n"," 'MRNA',\n"," 'MRO',\n"," 'MS',\n"," 'MSCI',\n"," 'MSFT',\n"," 'MSI',\n"," 'MTB',\n"," 'MTCH',\n"," 'MTD',\n"," 'MU',\n"," 'NCLH',\n"," 'NDAQ',\n"," 'NDSN',\n"," 'NEE',\n"," 'NEM',\n"," 'NFLX',\n"," 'NI',\n"," 'NKE',\n"," 'NOC',\n"," 'NOW',\n"," 'NRG',\n"," 'NSC',\n"," 'NTAP',\n"," 'NTRS',\n"," 'NUE',\n"," 'NVDA',\n"," 'NVR',\n"," 'NWL',\n"," 'NWS',\n"," 'NWSA',\n"," 'NXPI',\n"," 'O',\n"," 'ODFL',\n"," 'OGN',\n"," 'OKE',\n"," 'OMC',\n"," 'ON',\n"," 'ORCL',\n"," 'ORLY',\n"," 'OTIS',\n"," 'OXY',\n"," 'PARA',\n"," 'PAYC',\n"," 'PAYX',\n"," 'PCAR',\n"," 'PCG',\n"," 'PEAK',\n"," 'PEG',\n"," 'PEP',\n"," 'PFE',\n"," 'PFG',\n"," 'PG',\n"," 'PGR',\n"," 'PH',\n"," 'PHM',\n"," 'PKG',\n"," 'PKI',\n"," 'PLD',\n"," 'PM',\n"," 'PNC',\n"," 'PNR',\n"," 'PNW',\n"," 'POOL',\n"," 'PPG',\n"," 'PPL',\n"," 'PRU',\n"," 'PSA',\n"," 'PSX',\n"," 'PTC',\n"," 'PWR',\n"," 'PXD',\n"," 'PYPL',\n"," 'QCOM',\n"," 'QRVO',\n"," 'RCL',\n"," 'RE',\n"," 'REG',\n"," 'REGN',\n"," 'RF',\n"," 'RHI',\n"," 'RJF',\n"," 'RL',\n"," 'RMD',\n"," 'ROK',\n"," 'ROL',\n"," 'ROP',\n"," 'ROST',\n"," 'RSG',\n"," 'RTX',\n"," 'SBAC',\n"," 'SBUX',\n"," 'SCHW',\n"," 'SEDG',\n"," 'SEE',\n"," 'SHW',\n"," 'SIVB',\n"," 'SJM',\n"," 'SLB',\n"," 'SNA',\n"," 'SNPS',\n"," 'SO',\n"," 'SPG',\n"," 'SPGI',\n"," 'SRE',\n"," 'STE',\n"," 'STLD',\n"," 'STT',\n"," 'STX',\n"," 'STZ',\n"," 'SWK',\n"," 'SWKS',\n"," 'SYF',\n"," 'SYK',\n"," 'SYY',\n"," 'T',\n"," 'TAP',\n"," 'TDG',\n"," 'TDY',\n"," 'TECH',\n"," 'TEL',\n"," 'TER',\n"," 'TFC',\n"," 'TFX',\n"," 'TGT',\n"," 'TJX',\n"," 'TMO',\n"," 'TMUS',\n"," 'TPR',\n"," 'TRGP',\n"," 'TRMB',\n"," 'TROW',\n"," 'TRV',\n"," 'TSCO',\n"," 'TSLA',\n"," 'TSN',\n"," 'TT',\n"," 'TTWO',\n"," 'TXN',\n"," 'TXT',\n"," 'TYL',\n"," 'UAL',\n"," 'UDR',\n"," 'UHS',\n"," 'ULTA',\n"," 'UNH',\n"," 'UNP',\n"," 'UPS',\n"," 'URI',\n"," 'USB',\n"," 'V',\n"," 'VFC',\n"," 'VICI',\n"," 'VLO',\n"," 'VMC',\n"," 'VRSK',\n"," 'VRSN',\n"," 'VRTX',\n"," 'VTR',\n"," 'VTRS',\n"," 'VZ',\n"," 'WAB',\n"," 'WAT',\n"," 'WBA',\n"," 'WBD',\n"," 'WDC',\n"," 'WEC',\n"," 'WELL',\n"," 'WFC',\n"," 'WHR',\n"," 'WM',\n"," 'WMB',\n"," 'WMT',\n"," 'WRB',\n"," 'WRK',\n"," 'WST',\n"," 'WTW',\n"," 'WY',\n"," 'WYNN',\n"," 'XEL',\n"," 'XOM',\n"," 'XRAY',\n"," 'XYL',\n"," 'YUM',\n"," 'ZBH',\n"," 'ZBRA',\n"," 'ZION',\n"," 'ZTS',]"]},{"cell_type":"markdown","metadata":{"id":"lF-Jm_idFPiz"},"source":["#Data Cleaning & Engineering\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XWvQnoG9skHY"},"outputs":[],"source":["# This function inputs the total text of the filing, extracts the period for which it was filed, and\n","# returns the period ending year and month (yyyy-mm)\n","def get_period(url):\n","\n","  response = requests.get(url,headers={\"User-Agent\": \"Mozilla/5.0\"})\n","  text = response.text\n","  text = text.replace('\\n', '')\n","  text = text.replace('\\t','')\n","  date = re.findall(r\"CONFORMED PERIOD OF REPORT:\\s*(\\d{8})\", text)\n","\n","  if date:\n","      date = date[0]\n","      year = date[0:4]\n","      month = date[4:6]\n","      day = date[6:8]\n","      formatted_date = year + '-' + month + '-' + day\n","\n","  else:\n","    formatted_date = 'FLAG'\n","\n","  return formatted_date"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ecSUksQr15V"},"outputs":[],"source":["# Creates a file for each company in the S&P containing all 10-Q/Ks MD&A sections\n","\n","\n","for ticker in total_SP:\n","  df = pd.DataFrame(columns=['Ticker', 'Form Type','Period Ended','MD&A','url'])\n","\n","  query = {\n","      \"query\": {\n","          \"query_string\": {\n","              \"query\": \"(formType:\\\"10-Q\\\" OR formType:\\\"10-K\\\") AND ticker:(\"+ ticker +\")\"\n","          }\n","      },\n","      \"from\": \"0\",\n","      \"size\": \"1000\",\n","      \"sort\": [{ \"filedAt\": { \"order\": \"desc\" } }]\n","  }\n","\n","  response = queryApi.get_filings(query)\n","  filings = response['filings']\n","\n","  for index, doc in enumerate(filings,start=0):\n","\n","    print(ticker,index)\n","\n","    # Extract\n","    doc_type = doc['formType']\n","    url = doc['linkToTxt']\n","\n","    # Load\n","    df.loc[index, 'Ticker'] = ticker\n","    df.loc[index, 'Form Type'] = doc_type\n","    df.loc[index,'url'] = url\n","\n","    # Extract Period Date\n","    period = doc.get('periodOfReport')\n","    if period:\n","      df.loc[index, 'Period Ended'] = period\n","    else:\n","      df.loc[index, 'Period Ended'] = get_period(url)\n","    \n","    if doc_type == '10-K':\n","      # Extract 10k mdna\n","      text = extractApi.get_section(url,\"7\",\"text\")\n","      df.loc[index,'MD&A'] = text\n","\n","    elif doc_type == '10-Q':\n","      # Extract 10-Q mdna\n","      text = extractApi.get_section(url,\"part1item2\",\"text\")\n","      df.loc[index,'MD&A'] = text\n","    \n","  df.to_csv('/content/drive/MyDrive/Capstone-DataSets/Filings/'+ticker+'.csv')\n","  print(ticker + \" file saved\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oqb4EzdHvlyP"},"outputs":[],"source":["def getPercentReturn(old_val,new_val):\n","\n","    percent_change = ((new_val - old_val)/old_val) * 100\n","    return percent_change\n","\n","class DateError(Exception):\n","    pass\n","\n","def get_nearest_row(priceData,date):\n","  temp = priceData.loc[priceData['Date'] == date]\n","  if temp.empty:\n","    count = 0\n","    while temp.empty:\n","        date = pd.to_datetime(date)\n","        date = date + timedelta(days=1)\n","        date = date.strftime('%Y-%m-%d')\n","        temp = priceData.loc[priceData['Date'] == date]\n","        count += 1\n","        if count > 15:\n","          raise DateError('Matching Date not in dataset')\n","  return temp\n","\n","def getPeriodReturn(numDays,priceData, curRowIndex):\n","  if len(priceData) - 1 >= curRowIndex + numDays:\n","    old_price = priceData.loc[curRowIndex,'Adj Close']\n","    new_price = priceData.loc[curRowIndex + numDays,'Adj Close']\n","    return getPercentReturn(old_price, new_price)\n","  else:\n","    return \"NO DATA\" "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":164,"status":"ok","timestamp":1678565910780,"user":{"displayName":"Logan Deboo","userId":"05593846818062128905"},"user_tz":480},"id":"LWHNo5cAJLSZ","outputId":"caa06971-0b76-4e32-909f-205d900565a7"},"outputs":[{"data":{"text/plain":["100.0"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["getPercentReturn(1,2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HqOfw8XgraNu"},"outputs":[],"source":["# Inputs DF containing filings and their respective sentiment scores\n","# For each document, this method calculates the return over 90, 60, 30, 10, & 5 subsequent days and adds values into new columns\n","dateAnom = []\n","temp = pd.DataFrame()\n","def append_returns(ticker):\n","\n","    priceData = pd.read_csv('/content/drive/MyDrive/Capstone-DataSets/PriceData/'+ticker+'.csv',index_col=0)\n","    filing = pd.read_csv('/content/drive/MyDrive/Capstone-DataSets/Two_S_NewDate/'+ticker+'-2C.csv',index_col=0)\n","\n","    # Remove timestamp from date, if present\n","    priceData['Date'] = priceData['Date'].str.split(' ', n=1, expand=True)[0]\n","\n","    # Create new columns for returns. Note the conversion between *calendar* days and *trading* days. \n","    # One calendar month == 21 Trading days\n","    \n","    # 252 Trading Days\n","    filing['365 Day Return'] = None\n","\n","    # 126 Trading Days\n","    filing['180 Day Return'] = None\n","\n","    # 63 Trading Days\n","    filing['90 Day Return'] = None\n","\n","    # 42 Trading days\n","    filing['60 Day Return'] = None\n","\n","    # 21 Trading Days\n","    filing['30 Day Return'] = None\n","\n","    # 10 Trading Days\n","    filing['10 Day Return'] = None\n","\n","    # 5 Trading Days\n","    filing['5 Day Return'] = None\n","\n","\n","    for index, doc_row in filing.iterrows():\n","      print(index)\n","      # Return calculations must start from date filed, NOT quarter end \n","      # start_date = doc_row['Period Ended']\n","      start_date = doc_row['Filed At']\n","      print(\"FILED AT: \", start_date)\n","      \n","\n","      # Get row from price data closest to filing date (sometimes irregularities with filings)\n","      # If no price data, continue to next row. Very few occurences of this, hence the skip.\n","      try:\n","        matching_start_row = get_nearest_row(start_date,priceData)\n","      except DateError as d:\n","        continue\n","        \n","      \n","\n","      # Get price from date closest to filing date\n","      start_price = matching_start_row['Adj Close']\n","      print(\"START DATE FOR PRICE DATA: \", matching_start_row['Date'])\n","\n","      # Get index of the corresponding row\n","      i = matching_start_row.index[0]\n","      print(\"STARTING PRICE: \", matching_start_row['Adj Close'][i])\n","\n","\n","      filing.loc[index,'365 Day Return'] = getPeriodReturn(252,priceData,i)\n","      filing.loc[index,'180 Day Return'] = getPeriodReturn(126,priceData,i)\n","      filing.loc[index,'90 Day Return'] = getPeriodReturn(63,priceData,i)\n","      filing.loc[index,'60 Day Return'] = getPeriodReturn(42,priceData,i)\n","      filing.loc[index,'30 Day Return'] = getPeriodReturn(21,priceData,i)\n","      filing.loc[index,'10 Day Return'] = getPeriodReturn(10,priceData,i)\n","      filing.loc[index,'5 Day Return'] = getPeriodReturn(5,priceData,i)\n","    \n","    filing = filing.reindex(columns=['Ticker', 'Form Type', 'Period Ended','Filed At','S-yiyang','S-FLS', '365 Day Return','180 Day Return', '90 Day Return','60 Day Return','30 Day Return','10 Day Return', '5 Day Return','MD&A','url'])\n","    \n","    filing.to_csv('/content/drive/MyDrive/Capstone-DataSets/Two_S_NewDate/'+ticker+'-2C.csv')\n","    print(ticker,' FILE SAVED')\n","\n","    \n","    \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ntXzZmhVty6l"},"outputs":[],"source":["# Put all sentiment scores into single list\n","scores = []\n","for t in total_SP:\n","  temp = pd.read_csv('/content/drive/MyDrive/Capstone-DataSets/Two_Sentiment/'+t+'-2C.csv',index_col=0)\n","  for index, row in temp.iterrows():\n","    scores.append(row['S-FLS'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PUjtt85ZJdEZ"},"outputs":[],"source":["# Histogram of all sentiment values\n","fig = plt.figure(figsize=(12, 10))\n","ax = fig.add_subplot(1, 1, 1)\n","\n","ax.hist(scores, bins=70, color='orange', edgecolor='black')\n","ax.set_xlabel('Value')\n","ax.set_ylabel('Frequency')\n","ax.set_title('Total FLS Sentiment')\n","\n","ax.grid(True)\n","ax.set_axisbelow(True)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KKRtaocFK8eg"},"outputs":[],"source":["# Histogram of all negative sentiment values\n","fig = plt.figure(figsize=(12, 10))\n","ax = fig.add_subplot(1, 1, 1)\n","\n","ax.hist(neg, bins=70, color='orange', edgecolor='black')\n","ax.set_xlabel('Value')\n","ax.set_ylabel('Frequency')\n","ax.set_title('Negative FLS Sentiment Values')\n","\n","ax.grid(True)\n","ax.set_axisbelow(True)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XqLXwTq7LHP_"},"outputs":[],"source":["# Histogram of all positive sentiment values\n","fig = plt.figure(figsize=(12, 10))\n","ax = fig.add_subplot(1, 1, 1)\n","\n","ax.hist(pos, bins=70, color='orange', edgecolor='black')\n","ax.set_xlabel('Value')\n","ax.set_ylabel('Frequency')\n","ax.set_title('Positive Sentiment Values')\n","\n","ax.grid(True)\n","ax.set_axisbelow(True)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"34UAgaTKMgIx"},"outputs":[],"source":["# Anomaly: Some SEC files go back further than the ticker data. Tested multiple apis and websites which all match the ticker data. \n","# Not sure where these filigns are coming from. They consist of 744/37694 (~2%) of all documents\n","\n","revisit = pd.DataFrame()\n","count = 0\n","tickers = set()\n","for t in total_SP:\n","  temp = pd.read_csv('/content/drive/MyDrive/Capstone-DataSets/Completed/'+t+'-C.csv')\n","  for index, row in temp.iterrows():\n","    if pd.isnull(row['90 Day Return']) & pd.isnull(row['60 Day Return']) & pd.isnull(row['30 Day Return']) & pd.isnull(row['10 Day Return']) & pd.isnull(row['5 Day Return']):\n","      count += 1\n","      tickers.add(t)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":531,"status":"ok","timestamp":1677862132818,"user":{"displayName":"Logan Deboo","userId":"05593846818062128905"},"user_tz":480},"id":"1mUsNWlwNVWJ","outputId":"9622b595-063c-4705-c290-1a8f4c72e5bf"},"outputs":[{"data":{"text/plain":["500"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# Create and populate dictionary to sort tickers by GICS industry:\n","\n","no_price_data = ['FRC', 'GEHC', 'SBNY']\n","\n","industry_dict = {'Industrials':[],\n","             'Health Care':[],\n","             'Information Technology':[],\n","             'Communication Services':[],Communication\n","             'Consumer Staples':[],\n","             'Consumer Discretionary':[],\n","             'Financials':[],\n","             'Materials':[],\n","             'Utilities':[],\n","             'Real Estate': [],\n","             'Energy':[]\n","             }\n","\n","industry_csv = pd.read_csv('/content/drive/MyDrive/Capstone-DataSets/S&P-industries.csv',index_col=0)\n","\n","for index, row in industry_csv.iterrows():\n","  sector = str(row['Sector'])\n","  ticker = str(row['Tickers'])\n","  if ticker in no_price_data:\n","    continue \n","\n","  industry_dict[sector].append(ticker)\n","\n","# Veryify correct number of tickers - should be 500\n","keys = list(industry_dict.keys())\n","count = 0\n","for k in keys:\n","  count += len(industry_dict[k])\n","\n","count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6MUU4oRpRgKn"},"outputs":[],"source":["# Create histogram for sentiment of each industry\n","keys = list(industry_dict.keys())\n","for k in keys:\n","  scores = []\n","  for t in industry_dict[k]:\n","    temp = pd.read_csv('/content/drive/MyDrive/Capstone-DataSets/Two_Sentiment/'+t+'-2C.csv',index_col=0)\n","    for index, row in temp.iterrows():\n","      scores.append(row['S-FLS'])\n","\n","\n","  fig = plt.figure(figsize=(12, 10))\n","  ax = fig.add_subplot(1, 1, 1)\n","\n","  ax.hist(scores, bins=70, color='orange', edgecolor='black')\n","  ax.set_xlabel('Value')\n","  ax.set_ylabel('Frequency')\n","  ax.set_title(k+' FLS Sentiment')\n","\n","  ax.grid(True)\n","  ax.set_axisbelow(True)\n","\n","  plt.show()\n","\n","count\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-xib2OuA_kH"},"outputs":[],"source":["# Input: URL, Output: new column value with filing date\n","\n","total_SP = 'A'\n","for ticker in total_SP:\n","  \n","\n","  query = {\n","      \"query\": {\n","          \"query_string\": {\n","              \"query\": \"(formType:\\\"10-Q\\\" OR formType:\\\"10-K\\\") AND ticker:(\"+ ticker +\")\"\n","          }\n","      },\n","      \"from\": \"0\",\n","      \"size\": \"1000\",\n","      \"sort\": [{ \"filedAt\": { \"order\": \"asc\" } }]\n","  }\n","\n","  response = queryApi.get_filings(query)\n","\n","  test = pd.read_csv('/content/drive/MyDrive/Capstone-DataSets/Two_Sentiment/'+ticker+'-2C.csv',index_col=0)\n","  \n","  test['Filed At'] = None\n","\n","\n","  for index, row in test.iterrows():\n","\n","    # Get df index of first url to match\n","    matching_url = test['url'][index]\n","\n","    for i in range(len(response['filings'])):\n","      if response['filings'][i]['linkToTxt'] == matching_url:\n","        \n","        filed_at = response['filings'][i]['filedAt']\n","        filed_at_formatted = parse(filed_at).date()\n","        test.loc[index, 'Filed At'] = filed_at_formatted\n","        break\n","    \n","  \n","\n","  test.to_csv('/content/drive/MyDrive/Capstone-DataSets/Two_S_NewDate/'+ticker+'-2C.csv')\n","\n","\n","\n","  \n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_-aJbjECuxw"},"outputs":[],"source":["# Method inputs score type (either S-yiyang or S-FLS) winsorizes scores to be maximally within 3STDs and derives z scores\n","# Z score derivation is done using mean and std calculations performed exluding 0s\n","\n","def calc_z_scores(score_type):\n","\n","  master = pd.read_csv('/content/drive/MyDrive/Capstone-DataSets/master.csv',index_col = 0)\n","\n","  # Calculate sentiment score mean and std for winsorization\n","  scores = np.array(master[score_type])\n","  mean = np.mean(scores)\n","  std = np.std(scores)\n","  \n","  # Winsorize Scores to be maximally 3 stds from mean\n","  lower = mean - (3 * std)\n","  upper = mean + (3 * std)\n","  \n","  # winsorized_data = winsorize(scores, limits=(lower,upper))\n","\n","  upperCount = 0\n","  lowerCount = 0\n","\n","  # Winsorize outliers \n","  for val in scores:\n","    if val > upper:\n","      val = upper\n","      upperCount +=1\n","    elif val < lower:\n","      val = lower\n","      lowerCount+=1\n","  \n","  print(\"UPPER COUNT: \", upperCount)\n","  print(\"LOWER COUNT: \", lowerCount)\n","\n","  # Remove 0's to calculate mean and std for z scores\n","  no_zeros = scores[scores != 0]\n","  no_zero_mean = np.mean(no_zeros)\n","  no_zero_std = np.std(no_zeros)\n","\n","  # Loop over winsorized data array to calculate a z score for each value\n","  z_scores = []\n","  for x in scores:\n","    z = (x - no_zero_mean) / no_zero_std\n","    z_scores.append(z)\n","  \n","  assert(len(z_scores) == len(master))\n","\n","  if score_type == 'S-FLS':\n","    master['Z-Score-FLS'] = z_scores\n","  \n","  elif score_type == 'S-yiyang':\n","    master['Z-Score'] = z_scores\n","\n","  master.to_csv('/content/drive/MyDrive/Capstone-DataSets/master-v1.1.csv')  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQagtwglSeNl"},"outputs":[],"source":["# Method inputs a period, and score type (either S-yiyang or S-FLS) and performs a linear regression \n","def calc_t_score(period,score_type):\n","  \n","  master = pd.read_csv('/content/drive/MyDrive/Capstone-DataSets/master-v1.1.csv',index_col=0)\n","  col = str(period) + ' Day Return'\n","\n","  df = pd.DataFrame()\n","  df['Returns'] = master[col]\n","  df['Z-Scores'] = master[score_type]\n","\n","  df['Returns'] = pd.to_numeric(df.Returns, errors='coerce')\n","  df = df.dropna()\n","\n","  x = np.array(df['Z-Scores'])\n","  x = sm.add_constant(x)\n","  \n","  y = np.array(df['Returns'])\n","\n","\n","  model = sm.OLS(y,x).fit()\n","  print(model.summary())\n","  # tvals = list(model.tvalues)\n","\n","  # if len(tvals) == 1:\n","  #   return tvals[0]\n","  # else:\n","  #   return tvals[1]\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NHafzjJWVmT9"},"outputs":[],"source":["\n","periods = [365,180,90,60,30,10,5]\n","\n","stats = pd.DataFrame()\n","for p in periods:\n","  model = calc_t_score(p,'Z-Score')\n","\n","  period = str(p)+'D'\n","\n","  constant_coef = model.params[0]\n","  sentiment_coef = model.params[1]\n","\n","  constant_t = model.tvalues[0]\n","  sentiment_t = model.tvalues[1]\n","\n","\n","  r_squared = model.rsquared\n","  adj_r_squared = model.rsquared_adj\n","\n","  data = {'Sentiment Coef': sentiment_coef,\n","          'Sentiment t-stat': sentiment_t,\n","          'Const. Coef' : constant_coef,\n","          'Const. t-stat' : constant_t,\n","          'r squared' : r_squared,\n","          'adj. r squared': adj_r_squared\n","  }\n","\n","  df = pd.DataFrame(data, index=[period])\n","\n","  stats = pd.concat([stats,df],axis=0)\n","  "]},{"cell_type":"code","source":["# Read all sentiment scores into list to calc std and mean\n","all = []\n","count = 0\n","for ticker in total_SP:\n","  temp = pd.read_csv('/content/drive/MyDrive/Capstone-DataSets/Z-Scores/'+ticker+'.csv',index_col=0)\n","  print(ticker)\n","  count += len(temp)\n","  all.append(temp)\n","\n","master = pd.concat(all, axis=0)\n","assert(len(master) == count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZJQpGfNfufsq","executionInfo":{"status":"ok","timestamp":1681978429197,"user_tz":420,"elapsed":1493,"user":{"displayName":"Logan Deboo","userId":"05593846818062128905"}},"outputId":"b2f6c7b8-919e-4570-eb40-1cc1f8f93a2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-e9c3ba880cb5>:3: DtypeWarning: Columns (7,8,9,10,11,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv('/content/drive/MyDrive/Capstone-DataSets/master_minus_MD&A.csv',index_col=0)\n"]}]},{"cell_type":"code","source":["# Create and populate dictionary to sort tickers by GICS industry:\n","\n","no_price_data = ['FRC', 'GEHC', 'SBNY']\n","\n","industry_dict = {'Industrials':[],\n","             'Health Care':[],\n","             'Information Technology':[],\n","             'Communication Services':[],\n","             'Consumer Staples':[],\n","             'Consumer Discretionary':[],\n","             'Financials':[],\n","             'Materials':[],\n","             'Utilities':[],\n","             'Real Estate': [],\n","             'Energy':[]\n","             }\n","\n","industry_csv = pd.read_csv('/content/drive/MyDrive/Capstone-DataSets/S&P-industries.csv',index_col=0)\n","\n","for index, row in industry_csv.iterrows():\n","  sector = str(row['Sector'])\n","  ticker = str(row['Tickers'])\n","  if ticker in no_price_data:\n","    continue \n","\n","  industry_dict[sector].append(ticker)\n","\n","# Veryify correct number of tickers - should be 500\n","keys = list(industry_dict.keys())\n","count = 0\n","for k in keys:\n","  count += len(industry_dict[k])\n","\n","count"],"metadata":{"id":"ZPXH5jKnuib_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calc_z_scores(score_type):\n","\n","  master = pd.read_csv('/content/drive/MyDrive/Capstone-DataSets/master.csv',index_col = 0)\n","  \n","  # Calculate sentiment score mean and std for winsorization\n","  scores = np.array(master[score_type])\n","  mean = np.mean(scores)\n","  std = np.std(scores)\n","  \n","  # Winsorize Scores to be maximally 3 stds from mean\n","  lower = mean - (3 * std)\n","  upper = mean + (3 * std)\n","  \n","  # winsorized_data = winsorize(scores, limits=(lower,upper))\n","\n","  upperCount = 0\n","  lowerCount = 0\n","\n","  # Winsorize outliers \n","  for val in scores:\n","    if val > upper:\n","      val = upper\n","      upperCount +=1\n","    elif val < lower:\n","      val = lower\n","      lowerCount+=1\n","  \n","  print(\"UPPER COUNT: \", upperCount)\n","  print(\"LOWER COUNT: \", lowerCount)\n","\n","  # Remove 0's to calculate mean and std for z scores\n","  no_zeros = scores[scores != 0]\n","  no_zero_mean = np.mean(no_zeros)\n","  no_zero_std = np.std(no_zeros)\n","\n","  # Loop over winsorized data array to calculate a z score for each value\n","  z_scores = []\n","  for x in scores:\n","    z = (x - no_zero_mean) / no_zero_std\n","    z_scores.append(z)\n","  \n","  assert(len(z_scores) == len(master))\n","\n","  if score_type == 'S-FLS':\n","    master['Z-Score-FLS'] = z_scores\n","  \n","  elif score_type == 'S-yiyang':\n","    master['Z-Score'] = z_scores\n","\n","  master.to_csv('/content/drive/MyDrive/Capstone-DataSets/master-v1.1.csv')  "],"metadata":{"id":"11n8MSB4umC1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calc_t_score(period,score_type,industry):\n","\n","  col = str(period) + ' Day Return'\n","  df = pd.DataFrame()\n","\n","  if industry == 'All':\n","    df['Returns'] = master[col]\n","    df['ZScores'] = master[score_type]\n","  else:\n","    df = master[master['Ticker'].isin(list(industry_dict[industry]))]\n","    df['Returns'] = master[col]\n","    df['ZScores'] = master[score_type]\n","\n","  print(len(df))\n","\n","  df['Returns'] = pd.to_numeric(df.Returns, errors='coerce')\n","  df['ZScores'] = pd.to_numeric(df.ZScores, errors='coerce')\n","  df = df.dropna()\n","\n","  print(len(df))\n","\n","\n","  y = np.array(df['Returns'])\n","  x = np.array(df['ZScores'])\n","\n","  print(df)\n","\n","  print(type(x))\n","  print(type(y))\n","\n","  x = sm.add_constant(x)\n","\n","  model = sm.OLS(y,x).fit()\n","  print(model.summary())\n","  # tvals = list(model.tvalues)\n","\n","  # if len(tvals) == 1:\n","  #   return tvals[0]\n","  # else:\n","  #   return tvals[1]\n","  return model"],"metadata":{"id":"-qu5eTbvrfaR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calc_t_score_vol(period,score_type,industry):\n","\n","  col = str(period) + 'D Vol'\n","  df = pd.DataFrame()\n","\n","  if industry == 'All':\n","    df['Volatility'] = master[col]\n","    df['ZScores'] = master[score_type]\n","  else:\n","    df = master[master['Ticker'].isin(list(industry_dict[industry]))]\n","    df['Volatility'] = master[col]\n","    df['ZScores'] = master[score_type]\n","\n","  print(len(df))\n","\n","  df['Volatility'] = pd.to_numeric(df.Volatility, errors='coerce')\n","  df['ZScores'] = pd.to_numeric(df.ZScores, errors='coerce')\n","  df = df.dropna()\n","\n","  print(len(df))\n","\n","\n","  y = np.array(df['Volatility'])\n","  x = np.array(df['ZScores'])\n","\n","  print(df)\n","\n","  print(type(x))\n","  print(type(y))\n","\n","  x = sm.add_constant(x)\n","\n","  model = sm.OLS(y,x).fit()\n","  print(model.summary())\n","  # tvals = list(model.tvalues)\n","\n","  # if len(tvals) == 1:\n","  #   return tvals[0]\n","  # else:\n","  #   return tvals[1]\n","  return model"],"metadata":{"id":"2CxuaNTQrfc0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Method calculates a regression for each month beginning at the earliest month which information on 30 stocks is available\n","# Should start at 1994-04-01\n","\n","# For each month at time t:\n","#   - Get most recent z score for every stock and put into one column (independent variable)\n","#   - Get corresponding 30 day return and put into another column (dependent variable)\n","#   - Perform a regression and record stats\n","# Plot sentiment coefficient overtime "],"metadata":{"id":"j6J1Qc3erffS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DateError(Exception):\n","    pass\n","def get_nearest_row(date,priceData):\n","  temp = priceData.loc[priceData['Date'] == date]\n","  if temp.empty:\n","    count = 0\n","    while temp.empty:\n","        date = pd.to_datetime(date)\n","        date = date + timedelta(days=1)\n","        date = date.strftime('%Y-%m-%d')\n","        temp = priceData.loc[priceData['Date'] == date]\n","        count += 1\n","        if count > 15:\n","          raise DateError('Matching Date not in dataset')\n","  return temp"],"metadata":{"id":"xetkYsvyrfhL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def calc_rolling_regression():\n","from datetime import datetime\n","cur_date = '1994-04-01'\n","rolling_r = pd.DataFrame()\n","\n","while cur_date <= '2023-03-01':\n","\n","  print(\"CUR DATE: \", cur_date)\n","  \n","\n","  # Iterate backwards in time over master DF until row date is less than or equal to cur_date\n","  # Get the most recent document for each stock by:\n","    # Add every row of master into a temp DF if and only if, a row with its ticker has not already been added\n","  # Because the master df is sorted, we know that the first row we hit for each company will be the most recent\n","\n","  # df to hold independent and dependent variables in corresponding timeframe\n","  df = pd.DataFrame(columns=['Ticker','ZScore', 'Returns','Date'])\n","\n","  # FILTER FOR INDUSTRY IF WANTED\n","  master_temp = master[master['Ticker'].isin(list(industry_dict['Utilities']))]\n","\n","\n","  # IF filtered for industry, MUST iterate over master_temp, not master\n","  for index, row in master_temp.iterrows():\n","    if row['Filed At'] <= cur_date:\n","      ticker = row['Ticker']\n","      tickers = list(df['Ticker'])\n","      # The first ticker added to df will be the most recent (with respect to cur_date), so if there is already an entry for a specific ticker, do nothing on continue to next row\n","      if ticker in tickers:\n","        continue\n","      else:\n","        # Get price at cur_date\n","        # mask = master_price_data['Ticker'] == ticker\n","        # price_data = master_price_data[mask]\n","        # price_data = price_data.reset_index(drop=True)\n","        price_data = pd.read_csv('/content/drive/MyDrive/Capstone-DataSets/PriceData/'+ticker+'.csv',index_col=0)\n","        price_data['Date'] = price_data['Date'].apply(lambda x: x[:10])\n","        print(cur_date, ticker)\n","\n","        # If cur_date is before price history begins for specific stock, skip it.\n","        if cur_date < price_data.loc[0,'Date']:\n","          print(ticker,\" Skipped\")\n","          continue\n","\n","        # Get row from price data closest to cur_date\n","        price_row = get_nearest_row(cur_date,price_data)\n","        price_index = price_row.index[0]\n","\n","        # Get price at cur_date and cur_date + 21 (trading) days to prepare percentage change calculation\n","        old_val = price_data.loc[price_index,'Adj Close']\n","\n","        if (price_index + 21) > (len(price_data) - 1):\n","            monthly_return = 'NO DATA'\n","        else:\n","          new_val = price_data.loc[price_index+21,'Adj Close']\n","\n","          # Calculate Monthly return between cur_date and cur_date + 21\n","          monthly_return = ((new_val - old_val)/old_val) * 100\n","\n","        # Create df with single row to hold new calculations\n","        data = dict()\n","        print(row['Z-Score'])\n","        data = {'Ticker': row['Ticker'],\n","                'ZScore' : row['Z-Score'],\n","                'Returns' : monthly_return,\n","                'Date': row['Filed At']}\n","        temp = pd.DataFrame(data,index=[0])\n","\n","        # Append new row to df containing total month information\n","        df = pd.concat([df,temp],axis=0)\n","        test = df\n","  \n","  # Prepare df for regression \n","  # Remove rows with null values \n","  df['Returns'] = pd.to_numeric(df.Returns, errors='coerce')\n","  df['ZScore'] = pd.to_numeric(df.ZScore, errors='coerce')\n","  df = df.dropna()\n","\n","  # Construct independent and dependent variable vectors and perform regression\n","  y = np.array(df['Returns'])\n","  x = np.array(df['ZScore'])\n","\n","  x = sm.add_constant(x)\n","\n","  model = sm.OLS(y,x).fit()\n","\n","  # Exctact specific fields from regression results and place in returned df\n","  constant_coef = model.params[0]\n","  sentiment_coef = model.params[1]\n","\n","  constant_t = model.tvalues[0]\n","  sentiment_t = model.tvalues[1]\n","\n","\n","  r_squared = model.rsquared\n","  adj_r_squared = model.rsquared_adj\n","  \n","  data = dict()\n","  data = {'Date' : cur_date,\n","          'Sentiment Coef': sentiment_coef,\n","          'Sentiment t-stat': sentiment_t,\n","          'Const. Coef' : constant_coef,\n","          'Const. t-stat' : constant_t,\n","          'r squared' : r_squared,\n","          'adj. r squared': adj_r_squared\n","  }\n","\n","  df = pd.DataFrame(data,index=[0])\n","  rolling_r = pd.concat([rolling_r,df],axis=0)\n","\n","  date = datetime.strptime(cur_date, '%Y-%m-%d')\n","  cur_date = date + relativedelta(months=1)\n","  cur_date = cur_date.strftime('%Y-%m-%d')\n"],"metadata":{"id":"XqBsswu2rfjh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def getPercentReturn(old_val,new_val):\n","\n","    percent_change = ((new_val - old_val)/old_val) * 100\n","    return percent_change\n","\n","\n","def calc_volatility(df,date,period):\n","  # Get row (from price data) with closest date to filing date\n","  # If no matching row in price data, return nan\n","  try:\n","    row = get_nearest_row(date,df)\n","  except DateError:\n","    return np.nan\n","  price_index = row.index[0]\n","  \n","  # Array returns will hold daily returns over given period\n","  returns = np.array([])\n","  count = 0\n","  while count < period:\n","    old_val = df.loc[price_index,'Adj Close']\n","    if price_index + 1 > len(df)-1:\n","      break\n","\n","    new_val = df.loc[price_index+1,'Adj Close']\n","    returns = np.append(returns,getPercentReturn(old_val,new_val))\n","\n","    count+=1\n","    price_index +=1\n","  \n","  volatility = np.std(returns)\n","\n","  return volatility"],"metadata":{"id":"q3L0pdpgrfmI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calc_reg_plot(period,score_type,industry):\n","\n","  col = str(period) + ' Day Return'\n","  df = pd.DataFrame()\n","\n","  if industry == 'All':\n","    df['Returns'] = master[col]\n","    df['ZScores'] = master[score_type]\n","  else:\n","    df = master[master['Ticker'].isin(list(industry_dict[industry]))]\n","    df['Returns'] = master[col]\n","    df['ZScores'] = master[score_type]\n","\n","  print(len(df))\n","\n","  df['Returns'] = pd.to_numeric(df.Returns, errors='coerce')\n","  df['ZScores'] = pd.to_numeric(df.ZScores, errors='coerce')\n","  df = df.dropna()\n","\n","  print(len(df))\n","\n","\n","  y = np.array(df['Returns'])\n","  x = np.array(df['ZScores'])\n","\n","  # sns.regplot(x=x, y=y,scatter_kws={'s': 0.05},line_kws={\"color\":\"r\", \"linewidth\":0.5})\n","  # plt.xlabel(\"Adj. Sentiment Score\")\n","  # plt.ylabel(\"Return\")\n","  # plt.axhline(y=0, linestyle='-.', color='yellow', \"linewidth\":0.5})\n","\n","  sns.regplot(x=x, y=y, scatter_kws={'s': 0.05}, line_kws={\"color\": \"r\", \"linewidth\": 1.5})\n","\n","  # Add x and y axis labels\n","  plt.xlabel(\"Adj. Sentiment Score\")\n","  plt.ylabel(\"Return\")\n","  plt.title('365D Return Regression')\n","  plt.ylim(top=300)\n","\n","  # Add a horizontal dashed line at y=0 with yellow color and 0.5 line width\n","  plt.axhline(y=0, linestyle='--', color='r', linewidth=1)\n","  # remove the box around the graph\n","  plt.gca().spines['top'].set_visible(False)\n","  plt.gca().spines['right'].set_visible(False)\n","  plt.gca().spines['bottom'].set_visible(False)\n","  plt.gca().spines['left'].set_visible(False)\n","  # Show the plot\n","  plt.show()"],"metadata":{"id":"LNfWG8XBryrU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["time_periods = np.array(['365D', '180D', '90D', '60D', '30D', '10D','5D'])\n","returns = np.array([-16.7, -12, -10, -10.5, -9.9, -6,-1.98])\n","volatility = np.array([-24.5, -30.3, -32.6, -33, -32.3, -30.7,0])\n","\n","# Convert time_periods to categorical type\n","cat_time_periods = pd.Categorical(time_periods, categories=time_periods)\n","\n","# Create a figure and axis object\n","fig, ax = plt.subplots()\n","ax.grid(color='lightgray', alpha=0.7)\n","\n","# Set the width of the bars\n","bar_width = 0.25\n","\n","# Create the bar plot for returns\n","ax.bar(cat_time_periods.codes, returns, width=bar_width, label='Return', zorder=2)\n","\n","# Shift the position of the bars for volatility\n","ax.bar(cat_time_periods.codes + bar_width, volatility, width=bar_width, label='Volatility', zorder=2)\n","\n","# Set the labels and title for the plot\n","ax.set_xlabel('Period')\n","ax.set_ylabel('T-statistic value')\n","\n","\n","# Set the xticks to the categorical labels\n","ax.set_xticks(cat_time_periods.codes)\n","ax.set_xticklabels(cat_time_periods.categories)\n","\n","# Add a dashed red line to the plot\n","ax.axhline(y=-2, color='red', linestyle='-.', label='Significance Threshold |t| > 2')\n","\n","# Add a legend to the plot\n","ax.legend(loc='center right', bbox_to_anchor=(1.3, 1.3))\n","\n","# Display the plot\n","plt.show()"],"metadata":{"id":"1wPnYwler2HL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Define the data to be plotted\n","industries = ['Energy', 'Materials', 'Industrials', 'Consumer Discretionary', \n","              'Consumer Staples', 'Health Care', 'Financials', 'Information Technology', \n","              'Communication Services', 'Utilities', 'Real Estate','Total']\n","mean_values = [-2.67, -0.68, -8, -2.2, -2.37, -3.55, -0.18, -1.99, -1.82, -0.66, -6.29,-9.57]  # your own list of mean values\n","median_values = [-2.48, -3.28, -7.1, -2.64, -2.54, -3.77, -0.12, -2.24, -1.39, -1.01, -6.31,-9.97]  # your own list of median values\n","\n","# Set up the figure and axis with a larger size\n","fig, ax = plt.subplots(figsize=(12, 6))\n","\n","# Define the width of each bar\n","bar_width = 0.35\n","\n","# Plot the bars for each industry\n","x_pos = np.arange(len(industries))\n","ax.bar(x_pos - bar_width/2, mean_values, width=bar_width, label='Mean')\n","ax.bar(x_pos + bar_width/2, median_values, width=bar_width, label='Median')\n","\n","# Set the axis labels and title\n","ax.set_xlabel('Industry')\n","ax.set_ylabel('Value')\n","ax.set_title('Mean and Median Values by Industry')\n","\n","# Set the x-axis tick marks and labels\n","ax.set_xticks(x_pos)\n","ax.set_xticklabels(industries, rotation=45, ha='right')\n","\n","# Add a legend\n","ax.legend()\n","\n","# Show the plot\n","plt.show()"],"metadata":{"id":"T1gTSX-Ar41j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the data to be plotted\n","industries = ['Energy', 'Materials', 'Industrials', 'Consumer Discretionary', \n","              'Consumer Staples', 'Health Care', 'Financials', 'Information Technology', \n","              'Communication Services', 'Utilities', 'Real Estate','Total']\n","\n","\n","returns = [-2.67, -0.68, -8, -2.2, -2.37, -3.55, -0.18, -1.99, -1.82, -0.66, -6.29,-9.57]  # your own list of mean values\n","volatility = [-2.23, -5.8, -9.43, -16.72, -4.12, -12.73, -10.27, -16.59, -4.46, -3.9, -9.76,-30.54]  # your own list of median values\n","\n","# Sort the data by mean values in descending order\n","sorted_data = sorted(zip(industries, returns, volatility), key=lambda x: x[2], reverse=False)\n","industries, returns, volatility = zip(*sorted_data)\n","\n","# Set up the figure and axis with a larger size\n","fig, ax = plt.subplots(figsize=(12, 6))\n","\n","# Define the width of each bar\n","bar_width = 0.35\n","\n","# Plot the bars for each industry\n","x_pos = np.arange(len(industries))\n","ax.bar(x_pos - bar_width/2, returns, width=bar_width, label='Return',zorder=2)\n","ax.bar(x_pos + bar_width/2, volatility, width=bar_width, label='Volatility',zorder=2)\n","\n","# Set the axis labels and title\n","ax.set_ylabel('T-statistic value')\n","\n","\n","# Set the x-axis tick marks and labels\n","ax.set_xticks(x_pos)\n","ax.set_xticklabels(industries, rotation=45, ha='right')\n","\n","# Add a legend\n","ax.legend(framealpha=1)\n","ax.grid(True, linestyle='--', color='gray', alpha=0.5, zorder=1)\n","\n","# Show the plot\n","plt.show()"],"metadata":{"id":"xT_wuc67r6sT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["time_periods = np.array(['365D', '180D', '90D', '60D', '30D', '10D','5D'])\n","returns = np.array([-16.7, -12, -10, -10.5, -9.9, -6,-1.98])\n","volatility = np.array([-24.5, -30.3, -32.6, -33, -32.3, -30.7,0])\n","ma = np.array([-16.03, -10.66, -7.83, -8.09, -8.1, -5, -1.31])  # new array for \"MA\" values\n","\n","# Convert time_periods to categorical type\n","cat_time_periods = pd.Categorical(time_periods, categories=time_periods)\n","\n","# Create a figure and axis object\n","fig, ax = plt.subplots()\n","ax.grid(color='lightgray', alpha=0.7)\n","\n","# Set the width of the bars\n","bar_width = 0.25\n","\n","# Create the bar plot for returns\n","ax.bar(cat_time_periods.codes, returns, width=bar_width, label='Return', zorder=2)\n","\n","# Shift the position of the bars for volatility\n","ax.bar(cat_time_periods.codes + bar_width, volatility, width=bar_width, label='Volatility', zorder=2)\n","\n","# Add a new bar plot for MA values\n","ax.bar(cat_time_periods.codes + 2*bar_width, ma, width=bar_width, label='Return (MA Dif)', zorder=2)\n","\n","# Set the labels and title for the plot\n","ax.set_xlabel('Period')\n","ax.set_ylabel('T-statistic value')\n","\n","\n","# Set the xticks to the categorical labels\n","ax.set_xticks(cat_time_periods.codes + bar_width)\n","ax.set_xticklabels(cat_time_periods.categories)\n","\n","# Add a dashed red line to the plot\n","ax.axhline(y=-2, color='red', linestyle='-.', label='Significance Threshold |t| > 2')\n","\n","# Add a legend to the plot\n","ax.legend(loc='center right', bbox_to_anchor=(1.3, 1.3))\n","\n","# Display the plot\n","plt.show()"],"metadata":{"id":"lOGTj1afsE6P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qKEw3oATwN6K"},"source":["# Sentiment Analysis\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TJEKEfglp5Bv"},"outputs":[],"source":["def print_gpu_utilization():\n","    nvmlInit()\n","    handle = nvmlDeviceGetHandleByIndex(0)\n","    info = nvmlDeviceGetMemoryInfo(handle)\n","    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wlrviz8xqGKK"},"outputs":[],"source":["# Load and configure finBERT model to run on instance GPU. This portion of the code was executed using a Nvidia V100 via\n","# a custom Google GCE VM.\n","finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3).to(\"cuda\")\n","tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone',model_max_length=512)\n","classifier = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer,device=0,batch_size=32)\n","print_gpu_utilization()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TkIhB6TXqHgM"},"outputs":[],"source":["def get_sentiment(ticker):\n","  filing = pd.read_csv('/content/Filings/'+ticker+'.csv',index_col=0,encoding='utf-8')\n","  filing = filing.dropna()\n","  filing = filing.reset_index(drop=True)\n","  filing['MD&A'] = filing['MD&A'].replace(r'\\n',' ',regex=True)\n","  filing['S-yiyang'] = None\n","\n","\n","  # finbert Sentence Level Granularity\n","  for index, row in filing.iterrows():  \n","\n","    # Tokenize mdna section into individual sentences\n","    text = filing['MD&A'][index]\n","    sentences = nltk.sent_tokenize(text)\n","\n","    # Initalize counters\n","    neg_count = 0\n","    neutral_count = 0\n","    pos_count = 0\n","\n","    for s in sentences:\n","      # Get sentiment of sentence\n","      sentiment = classifier(s,padding=True, truncation=True)\n","\n","      # Count occurrences of pos/neutral/neg\n","      if sentiment[0]['label'] == 'Positive':\n","        pos_count = pos_count + 1\n","\n","      if sentiment[0]['label'] == 'Neutral':\n","        neutral_count = neutral_count + 1\n","\n","      if sentiment[0]['label'] == 'Negative':\n","        neg_count = neg_count + 1\n","\n","    # Calculate final sentiment score\n","    total = neg_count + neutral_count + pos_count\n","    percent_neg = neg_count / total\n","    percent_pos = pos_count / total\n","    sentiment_score = (percent_pos - percent_neg)\n","\n","\n","\n","    # Push score to DF \n","    filing.loc[index,'S-yiyang'] = sentiment_score\n","    print(index,\" \", ticker, \" \",sentiment_score)\n","  \n","  # filing['S-yiyang'] = scores\n","  filing.to_csv('/content/Filings_and_Sentiment/'+ticker+'-S.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k5C2QX3GZh4R"},"outputs":[],"source":["# Verify all files have been downloaded\n","todo = []\n","for ticker in total_SP:\n","  try:\n","    test = pd.read_csv('/content/drive/MyDrive/Capstone-DataSets/Two_Sentiment/'+ticker+'-2C.csv')\n","  except FileNotFoundError:\n","    todo.append(ticker)\n","    print(ticker)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fARVJy6zfz0O"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nDogRZGBCs45"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gSPhqnDWm6gm"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UR8xiRXwvTgF"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1A4PsuRSgnM07etmeHs_QqoCO-kgfFUam","timestamp":1682145778289}],"mount_file_id":"1A4PsuRSgnM07etmeHs_QqoCO-kgfFUam","authorship_tag":"ABX9TyNh7lJ0fpUPgBntGA1Ylcw1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}